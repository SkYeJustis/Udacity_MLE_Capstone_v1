{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Baseline Model - MLP - 1 Hidden Layer](#baseline-mlp-1-hidden-layer)\n",
    "* [Baseline Model - MLP - Image Augmentation](#mlp-img-aug)\n",
    "* [MLP - 2 Hidden Layers](#mlp-2-hidden-layer)\n",
    "* [CNN - 3 Convolution layers](#cnn-1)\n",
    "* [CNN - Learning rate change + Increased epochs](#cnn-1a)\n",
    "* [CNN - 4 Convolution layers](#cnn-2)\n",
    "* [CNN - Transfer learning - Vgg16](#cnn-3)\n",
    "* [Final Validation](#final-validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset directories\n",
    "DATA_DIR = '../chest_xray_mini/'\n",
    "TRAIN = \"{0}{1}\".format(DATA_DIR, 'train')\n",
    "TEST = \"{0}{1}\".format(DATA_DIR, 'test')\n",
    "VALID = \"{0}{1}\".format(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import Accuracy, Precision, Recall\n",
    "\n",
    "def thresholded_output_transform(output):\n",
    "    y_pred, y = output\n",
    "    y_pred = torch.round(y_pred)\n",
    "    return y_pred, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, model_path, test_loader):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    \n",
    "    binary_accuracy = Accuracy(thresholded_output_transform)\n",
    "    precision = Precision(thresholded_output_transform)\n",
    "    recall = Recall(thresholded_output_transform)\n",
    "    \n",
    "    precision_1 = Precision(average=False)\n",
    "    recall_1 = Recall(average=False)\n",
    "    F1 = (precision_1 * recall_1 * 2 / (precision_1 + recall_1)).mean()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if use_cuda:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            binary_accuracy.update((predicted, labels))\n",
    "            precision.update((predicted, labels))\n",
    "            recall.update((predicted, labels))\n",
    "            \n",
    "            precision_1.update((predicted, labels))\n",
    "            recall_1.update((predicted, labels))\n",
    "            \n",
    "        prec_val = precision.compute().item()\n",
    "        rec_val = recall.compute().item()\n",
    "        \n",
    "        print('Model Accuracy : ', binary_accuracy.compute())\n",
    "        print('Model Precision : ', prec_val )\n",
    "        print('Model Recall : ', rec_val)\n",
    "        print('Model F1 : ', F1.compute().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epoch, model_path, model, train_loader, valid_loader, transfer_learning=False):\n",
    "    min_val_loss = np.Inf\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for e in range(n_epoch):\n",
    "        running_loss = 0\n",
    "        val_loss = 0\n",
    "        # train mode\n",
    "        for images, labels in train_loader:\n",
    "            if use_cuda:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "            # zero grad\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # testing set\n",
    "        for images, labels in valid_loader:\n",
    "            if use_cuda:\n",
    "                images, labels = images.cuda(), labels.cuda()\n",
    "            if not transfer_learning:\n",
    "                print(\"Evaluating model...\")\n",
    "                model.eval()\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        if not transfer_learning:\n",
    "            print(\"Training model...\")\n",
    "            model.train()\n",
    "\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_val_loss = val_loss / len(valid_loader.dataset)\n",
    "        print('Epoch {}, train loss : {}, validation loss :{}'.format(e, epoch_train_loss, epoch_val_loss))\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        if epoch_val_loss <= min_val_loss:\n",
    "            print('Validation loss decreased {} -> {}. Saving model...'.format(min_val_loss, epoch_val_loss))\n",
    "            min_val_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model - Multi-layer Perceptron - 1 Hidden Layer <a class=\"anchor\" id=\"baseline-mlp-1-hidden-layer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      #transforms.RandomRotation(10),\n",
    "                                      #transforms.RandomHorizontalFlip(), # Randomly flip and rotate\n",
    "                                      transforms.ToTensor(),\n",
    "                                      ##transforms.CenterCrop(224),\n",
    "                                      transforms.Normalize((.5,.5,.5),\n",
    "                                                           (.5,.5,.5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                     #transforms.Grayscale(num_output_channels=1),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    #transforms.CenterCrop(224),\n",
    "                                    transforms.Normalize((.5,.5,.5),\n",
    "                                                         (.5,.5,.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(TRAIN, train_transform)\n",
    "test_data = datasets.ImageFolder(TEST, test_transform)\n",
    "valid_data = datasets.ImageFolder(VALID, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=512, \n",
    "                          num_workers=2, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=512, \n",
    "                         num_workers=2, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=512, \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(3*224*224, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 2)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # flatten image input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.size())\n",
    "        #print(self.fc1.weight.size())\n",
    "        \n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=150528, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 0, train loss : 0.6787442088127136, validation loss :0.6223978140415289\n",
      "Validation loss decreased inf -> 0.6223978140415289. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 1, train loss : 0.6415060248374939, validation loss :0.5789011426461048\n",
      "Validation loss decreased 0.6223978140415289 -> 0.5789011426461048. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 2, train loss : 0.5796319642066956, validation loss :0.5490971513283558\n",
      "Validation loss decreased 0.5789011426461048 -> 0.5490971513283558. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 3, train loss : 0.66183030128479, validation loss :0.5524806456688123\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 4, train loss : 0.5651984868049622, validation loss :0.479512883302493\n",
      "Validation loss decreased 0.5490971513283558 -> 0.479512883302493. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 5, train loss : 0.4954883394241333, validation loss :0.553883630495805\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 6, train loss : 0.6711929092407226, validation loss :0.49088373398169494\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 7, train loss : 0.44864882731437683, validation loss :0.4148832720059615\n",
      "Validation loss decreased 0.479512883302493 -> 0.4148832720059615. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 8, train loss : 0.3664274971485138, validation loss :0.40635125988569015\n",
      "Validation loss decreased 0.4148832720059615 -> 0.40635125988569015. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 9, train loss : 0.4487936015129089, validation loss :0.5428333878517151\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "model_path = 'mlp_1.pt'\n",
    "\n",
    "train(n_epoch, model_path, model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.8141025641025641\n",
      "Model Precision :  0.7818930041152263\n",
      "Model Recall :  0.9743589743589743\n",
      "Model F1 :  0.8675799086757991\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "calculate_metrics(model, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron - Image Augmentation <a class=\"anchor\" id=\"mlp-img-aug\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.RandomRotation(10),\n",
    "                                      transforms.RandomHorizontalFlip(), # Randomly flip and rotate\n",
    "                                      transforms.ToTensor(),\n",
    "                                      ##transforms.CenterCrop(224),\n",
    "                                      transforms.Normalize((.5,.5,.5),\n",
    "                                                           (.5,.5,.5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    #transforms.CenterCrop(224),\n",
    "                                    transforms.Normalize((.5,.5,.5),\n",
    "                                                         (.5,.5,.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(TRAIN, train_transform)\n",
    "test_data = datasets.ImageFolder(TEST, test_transform)\n",
    "valid_data = datasets.ImageFolder(VALID, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=512, \n",
    "                          num_workers=2, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=512, \n",
    "                         num_workers=2, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=512, \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=150528, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 0, train loss : 0.6795289363861085, validation loss :0.6308514124307877\n",
      "Validation loss decreased inf -> 0.6308514124307877. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 1, train loss : 0.6321136407852173, validation loss :0.5929280106837933\n",
      "Validation loss decreased 0.6308514124307877 -> 0.5929280106837933. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 2, train loss : 0.6812675909996033, validation loss :0.6055843310478406\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 3, train loss : 0.5962149958610534, validation loss :0.5640699038138757\n",
      "Validation loss decreased 0.5929280106837933 -> 0.5640699038138757. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 4, train loss : 0.5067292184829711, validation loss :0.4867049853006999\n",
      "Validation loss decreased 0.5640699038138757 -> 0.4867049853006999. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 5, train loss : 0.674034019947052, validation loss :0.5965427099130093\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 6, train loss : 0.6383845324516296, validation loss :0.5465968205378606\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 7, train loss : 0.4710044462680817, validation loss :0.4542226714965625\n",
      "Validation loss decreased 0.4867049853006999 -> 0.4542226714965625. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 8, train loss : 0.43629459738731385, validation loss :0.4693620877388196\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 9, train loss : 0.6584853720664978, validation loss :0.48135439860515106\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "model_path = 'mlp_1a.pth'\n",
    "\n",
    "train(n_epoch, model_path, model,  train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.8333333333333334\n",
      "Model Precision :  0.8647959183673469\n",
      "Model Recall :  0.8692307692307693\n",
      "Model F1 :  0.867007672634271\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "calculate_metrics(model, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not significantly helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron - 2 Hidden Layers <a class=\"anchor\" id=\"mlp-2-hidden-layer\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      #transforms.RandomRotation(10),\n",
    "                                      #transforms.RandomHorizontalFlip(), # Randomly flip and rotate\n",
    "                                      transforms.ToTensor(),\n",
    "                                      ##transforms.CenterCrop(224),\n",
    "                                      transforms.Normalize((.5,.5,.5),\n",
    "                                                           (.5,.5,.5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    #transforms.CenterCrop(224),\n",
    "                                    transforms.Normalize((.5,.5,.5),\n",
    "                                                         (.5,.5,.5))])\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN, train_transform)\n",
    "test_data = datasets.ImageFolder(TEST, test_transform)\n",
    "valid_data = datasets.ImageFolder(VALID, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=256, \n",
    "                          num_workers=2, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, \n",
    "                         num_workers=2, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=256,\n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(3*224*224, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "        self.fc4 = nn.Linear(64, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # flatten image input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)   \n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=150528, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 0, train loss : 0.6796749057769775, validation loss :0.666423513339116\n",
      "Validation loss decreased inf -> 0.666423513339116. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 1, train loss : 0.6387327246665955, validation loss :0.6209630553538983\n",
      "Validation loss decreased 0.666423513339116 -> 0.6209630553538983. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 2, train loss : 0.5986431317329407, validation loss :0.5815777488243885\n",
      "Validation loss decreased 0.6209630553538983 -> 0.5815777488243885. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 3, train loss : 0.5395042109489441, validation loss :0.5292323720760834\n",
      "Validation loss decreased 0.5815777488243885 -> 0.5292323720760834. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 4, train loss : 0.5236451826095581, validation loss :0.5069764684408139\n",
      "Validation loss decreased 0.5292323720760834 -> 0.5069764684408139. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 5, train loss : 0.4604601769447327, validation loss :0.44819746414820355\n",
      "Validation loss decreased 0.5069764684408139 -> 0.44819746414820355. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 6, train loss : 0.4146121563911438, validation loss :0.4735065996646881\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 7, train loss : 0.4270205454826355, validation loss :0.4240996944598662\n",
      "Validation loss decreased 0.44819746414820355 -> 0.4240996944598662. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 8, train loss : 0.4221348006725311, validation loss :0.4473714851416074\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 9, train loss : 0.36827654600143434, validation loss :0.40323023383434003\n",
      "Validation loss decreased 0.4240996944598662 -> 0.40323023383434003. Saving model...\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "model_path = 'mlp_2.pt'\n",
    "            \n",
    "train(n_epoch, model_path, model,  train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.8044871794871795\n",
      "Model Precision :  0.7757201646090535\n",
      "Model Recall :  0.9666666666666667\n",
      "Model F1 :  0.8607305936073061\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "calculate_metrics(model, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network - 3 convolutional layers <a class=\"anchor\" id=\"cnn-1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      #transforms.RandomRotation(10),\n",
    "                                      #transforms.RandomHorizontalFlip(), # Randomly flip and rotate\n",
    "                                      transforms.ToTensor(),\n",
    "                                      ##transforms.CenterCrop(224),\n",
    "                                      transforms.Normalize((.5,.5,.5),\n",
    "                                                           (.5,.5,.5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    #transforms.CenterCrop(224),\n",
    "                                    transforms.Normalize((.5,.5,.5),\n",
    "                                                         (.5,.5,.5))])\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN, train_transform)\n",
    "test_data = datasets.ImageFolder(TEST, test_transform)\n",
    "valid_data = datasets.ImageFolder(VALID, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, \n",
    "                          num_workers=2, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64,\n",
    "                         num_workers=2, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=64, \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(224 * 224, 512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        #print(x.size())\n",
    "        #print(self.fc1.weight.size())\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 0, train loss : 0.6909088349342346, validation loss :0.6937172305889618\n",
      "Validation loss decreased inf -> 0.6937172305889618. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 1, train loss : 0.6850294981002808, validation loss :0.6899292331475478\n",
      "Validation loss decreased 0.6937172305889618 -> 0.6899292331475478. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 2, train loss : 0.6767686533927918, validation loss :0.6876752009758582\n",
      "Validation loss decreased 0.6899292331475478 -> 0.6876752009758582. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 3, train loss : 0.6638892226219177, validation loss :0.6711350159767346\n",
      "Validation loss decreased 0.6876752009758582 -> 0.6711350159767346. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 4, train loss : 0.6411803660392761, validation loss :0.664151424016708\n",
      "Validation loss decreased 0.6711350159767346 -> 0.664151424016708. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 5, train loss : 0.5936593813896179, validation loss :0.5869775231067951\n",
      "Validation loss decreased 0.664151424016708 -> 0.5869775231067951. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 6, train loss : 0.4959095311164856, validation loss :0.4727727831938328\n",
      "Validation loss decreased 0.5869775231067951 -> 0.4727727831938328. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 7, train loss : 0.3922229495048523, validation loss :0.40792179566163284\n",
      "Validation loss decreased 0.4727727831938328 -> 0.40792179566163284. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 8, train loss : 0.4474165699481964, validation loss :0.39699670596000475\n",
      "Validation loss decreased 0.40792179566163284 -> 0.39699670596000475. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 9, train loss : 0.26966167521476747, validation loss :0.358255909039424\n",
      "Validation loss decreased 0.39699670596000475 -> 0.358255909039424. Saving model...\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "model_path = 'cnn_1.pt'\n",
    "\n",
    "train(n_epoch, model_path, model,  train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.8237179487179487\n",
      "Model Precision :  0.8783783783783784\n",
      "Model Recall :  0.8333333333333334\n",
      "Model F1 :  0.855263157894737\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "calculate_metrics(model, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network - Learning rate change + Increased epochs <a class=\"anchor\" id=\"cnn-1a\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      #transforms.RandomRotation(20),\n",
    "                                      #transforms.RandomHorizontalFlip(),\n",
    "                                      #transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((.5,.5,.5),\n",
    "                                                           (.5,.5,.5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    #transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((.5,.5,.5),\n",
    "                                                         (.5,.5,.5))])\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN, train_transform)\n",
    "test_data = datasets.ImageFolder(TEST, test_transform)\n",
    "valid_data = datasets.ImageFolder(VALID, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, \n",
    "                          num_workers=2, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, \n",
    "                         num_workers=2, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=64, \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(224 * 224, 512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add sequence of convolutional and max pooling layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        #print(x.size())\n",
    "        #print(self.fc1.weight.size())\n",
    "        # add 1st hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=50176, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 0, train loss : 0.6931862454414368, validation loss :0.684100560652904\n",
      "Validation loss decreased inf -> 0.684100560652904. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 1, train loss : 0.6920372748374939, validation loss :0.6850152015686035\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 2, train loss : 0.6907897334098816, validation loss :0.6856062519244659\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 3, train loss : 0.6892915358543396, validation loss :0.6862116639430706\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 4, train loss : 0.6889471759796143, validation loss :0.6865808000931373\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 5, train loss : 0.6871918544769288, validation loss :0.6869198053311079\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 6, train loss : 0.6865421299934387, validation loss :0.6869691396370913\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 7, train loss : 0.6857534465789795, validation loss :0.6869548161824545\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 8, train loss : 0.6844236731529236, validation loss :0.6869629468673315\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 9, train loss : 0.6837626113891602, validation loss :0.6872149919852232\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 10, train loss : 0.6827470893859864, validation loss :0.6868698535821377\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 11, train loss : 0.6815949869155884, validation loss :0.6867885895264454\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 12, train loss : 0.6811440978050232, validation loss :0.6867840473468487\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 13, train loss : 0.6802431287765502, validation loss :0.6866353352864584\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 14, train loss : 0.6788414077758789, validation loss :0.686256115253155\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 15, train loss : 0.6775861344337464, validation loss :0.685559304860922\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 16, train loss : 0.676703622341156, validation loss :0.6853572328885397\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 17, train loss : 0.6758364901542664, validation loss :0.6847414970397949\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 18, train loss : 0.6743844780921936, validation loss :0.684301706460806\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 19, train loss : 0.6733329939842224, validation loss :0.6834232394511883\n",
      "Validation loss decreased 0.684100560652904 -> 0.6834232394511883. Saving model...\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 20\n",
    "model_path = 'cnn_1a.pt'\n",
    "\n",
    "train(n_epoch, model_path, model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.6426282051282052\n",
      "Model Precision :  0.9513513513513514\n",
      "Model Recall :  0.4512820512820513\n",
      "Model F1 :  0.6121739130434783\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "calculate_metrics(model, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not significantly helpful - need more epochs and compute resources to make use of parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network -  4 Convolution layers <a class=\"anchor\" id=\"cnn-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      #transforms.RandomRotation(20),\n",
    "                                      #transforms.RandomHorizontalFlip(),\n",
    "                                      #transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((.5,.5,.5),\n",
    "                                                           (.5,.5,.5))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    #transforms.CenterCrop(224),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((.5,.5,.5),\n",
    "                                                         (.5,.5,.5))])\n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN, train_transform)\n",
    "test_data = datasets.ImageFolder(TEST, test_transform)\n",
    "valid_data = datasets.ImageFolder(VALID, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, \n",
    "                          num_workers=2, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, \n",
    "                         num_workers=2, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=64, \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding = 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 4096\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        # flatten image input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=12544, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 0, train loss : 0.6930508561134339, validation loss :0.6871998722736652\n",
      "Validation loss decreased inf -> 0.6871998722736652. Saving model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 1, train loss : 0.6915600066184997, validation loss :0.6886185331222339\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 2, train loss : 0.6906518998146057, validation loss :0.6897161266742609\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 3, train loss : 0.6897656378746033, validation loss :0.6910743896777813\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 4, train loss : 0.6888816766738891, validation loss :0.6915405346797063\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 5, train loss : 0.6875775003433228, validation loss :0.6912080370462858\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 6, train loss : 0.6868011174201966, validation loss :0.6916206020575303\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 7, train loss : 0.6854925031661987, validation loss :0.6910276947877346\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 8, train loss : 0.6839824047088623, validation loss :0.6907850152406937\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Evaluating model...\n",
      "Training model...\n",
      "Epoch 9, train loss : 0.6825944910049438, validation loss :0.691403268239437\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "model_path = 'cnn_2.pt'\n",
    "\n",
    "train(n_epoch, model_path, model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.625\n",
      "Model Precision :  0.625\n",
      "Model Recall :  1.0\n",
      "Model F1 :  0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "calculate_metrics(model, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network - Transfer learning - vgg16 <a class=\"anchor\" id=\"cnn-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      #transforms.RandomRotation(10),\n",
    "                                      #transforms.RandomHorizontalFlip(), # Randomly flip and rotate\n",
    "                                      transforms.ToTensor()])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                    transforms.ToTensor()])\n",
    "                                     \n",
    "\n",
    "train_data = datasets.ImageFolder(TRAIN, train_transform)\n",
    "test_data = datasets.ImageFolder(TEST, test_transform)\n",
    "valid_data = datasets.ImageFolder(VALID, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, \n",
    "                          num_workers=2, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_data, batch_size=128, \n",
    "                         num_workers=2, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=128, \n",
    "                          num_workers=2, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "1000\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/udacity/deep-learning-v2-pytorch/blob/master/transfer-learning/Transfer_Learning_Solution.ipynb\n",
    "# https://www.kaggle.com/parry2020/chest-xray-pneumonia-pytorch-vgg16\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "print(model.classifier[6].in_features) \n",
    "print(model.classifier[6].out_features)\n",
    "\n",
    "# Freeze training for all \"features\" layers\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Final classification layer\n",
    "n_inputs = model.classifier[6].in_features\n",
    "\n",
    "last_layer = nn.Linear(n_inputs, 2)\n",
    "\n",
    "model.classifier[6] = last_layer\n",
    "\n",
    "# check to see that your last layer produces the expected number of outputs\n",
    "print(model.classifier[6].out_features)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss : 0.7797216880321503, validation loss :0.4720657284443195\n",
      "Validation loss decreased inf -> 0.4720657284443195. Saving model...\n",
      "Epoch 1, train loss : 0.2994156186580658, validation loss :0.4361172463649359\n",
      "Validation loss decreased 0.4720657284443195 -> 0.4361172463649359. Saving model...\n",
      "Epoch 2, train loss : 0.24825574326515198, validation loss :0.43003998200098675\n",
      "Validation loss decreased 0.4361172463649359 -> 0.43003998200098675. Saving model...\n",
      "Epoch 3, train loss : 0.21226462197303772, validation loss :0.4294001704607254\n",
      "Validation loss decreased 0.43003998200098675 -> 0.4294001704607254. Saving model...\n",
      "Epoch 4, train loss : 0.1798077504634857, validation loss :0.41744085535024983\n",
      "Validation loss decreased 0.4294001704607254 -> 0.41744085535024983. Saving model...\n",
      "Epoch 5, train loss : 0.15451629900932312, validation loss :0.46953692879432285\n",
      "Validation loss decreased 0.41744085535024983 -> 0.46953692879432285. Saving model...\n",
      "Epoch 6, train loss : 0.14158534395694733, validation loss :0.3867588532276643\n",
      "Validation loss decreased 0.46953692879432285 -> 0.3867588532276643. Saving model...\n",
      "Epoch 7, train loss : 0.12954616582393647, validation loss :0.4183158553563632\n",
      "Validation loss decreased 0.3867588532276643 -> 0.4183158553563632. Saving model...\n",
      "Epoch 8, train loss : 0.12075857615470886, validation loss :0.35217957542492795\n",
      "Validation loss decreased 0.4183158553563632 -> 0.35217957542492795. Saving model...\n",
      "Epoch 9, train loss : 0.11697629737854004, validation loss :0.4363877368278993\n",
      "Validation loss decreased 0.35217957542492795 -> 0.4363877368278993. Saving model...\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "model_path = 'cnn_3.pt'\n",
    "\n",
    "train(n_epoch, model_path, model, train_loader, test_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.8301282051282052\n",
      "Model Precision :  0.8021276595744681\n",
      "Model Recall :  0.9666666666666667\n",
      "Model F1 :  0.8767441860465117\n"
     ]
    }
   ],
   "source": [
    "calculate_metrics(model, model_path, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final validation <a class=\"anchor\" id=\"final-validation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The vgg model worked best:\n",
    "    * The model setting was kept in the `model` variable and used to download the metric path to calculate the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.875\n",
      "Model Precision :  0.8\n",
      "Model Recall :  1.0\n",
      "Model F1 :  0.888888888888889\n"
     ]
    }
   ],
   "source": [
    "model_path = \"cnn_3.pt\"\n",
    "calculate_metrics(model, model_path, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assess validation set with baseline too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(3*224*224, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 2)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=.2)\n",
    "  \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # flatten image input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.size())\n",
    "        #print(self.fc1.weight.size())\n",
    "        \n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy :  0.5\n",
      "Model Precision :  0.5\n",
      "Model Recall :  1.0\n",
      "Model F1 :  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "model_path = 'mlp_1.pt'\n",
    "calculate_metrics(model, model_path, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
